---
# AWS Site Playbook - Complete server provisioning and configuration
# This combines aws_provision.yml and aws_configure.yml into a single workflow

- name: Provision AWS EC2 Server
  hosts: localhost
  gather_facts: false
  become: false
  vars:
    # Use remo venv Python which has boto3 installed
    ansible_python_interpreter: "{{ lookup('env', 'HOME') }}/.remo/.venv/bin/python"
  environment:
    # Let boto3 handle credentials via environment variables
    AWS_PROFILE: "{{ aws_profile | default('') }}"
    AWS_ACCESS_KEY_ID: "{{ '' if (aws_profile | default('')) else (aws_access_key_id | default('')) }}"
    AWS_SECRET_ACCESS_KEY: "{{ '' if (aws_profile | default('')) else (aws_secret_access_key | default('')) }}"
    AWS_DEFAULT_REGION: "{{ aws_region | default('us-west-2') }}"
  tasks:
    - name: Include AWS server role
      ansible.builtin.include_role:
        name: aws_server

- name: Configure AWS EC2 Server
  hosts: provisioned
  become: true
  gather_facts: false
  vars:
    # Pass storage info from localhost to the provisioned host
    # EBS (block storage) - used for direct access
    remo_volume_device: "{{ hostvars['localhost']['aws_ebs_device_path'] | default('') }}"

  pre_tasks:
    # --- TEMPORARY DEBUG: remove after diagnosing SSM 403 ---
    - name: "DEBUG: Show SSH connection args for this host"
      delegate_to: localhost
      become: false
      ansible.builtin.debug:
        msg:
          ansible_host: "{{ ansible_host | default('NOT SET') }}"
          ansible_ssh_args: "{{ ansible_ssh_args | default('NOT SET') }}"
          ansible_ssh_common_args: "{{ ansible_ssh_common_args | default('NOT SET') }}"
          ansible_ssh_extra_args: "{{ ansible_ssh_extra_args | default('NOT SET') }}"
          env_AWS_PROFILE: "{{ lookup('env', 'AWS_PROFILE') | default('NOT SET', true) }}"

    - name: "DEBUG: Check SSM agent status via AWS API"
      delegate_to: localhost
      become: false
      ansible.builtin.shell: |
        python3 << 'PYEOF'
        import boto3, json, os
        region = os.environ.get('AWS_DEFAULT_REGION', '{{ hostvars["localhost"]["aws_region"] | default("us-west-2") }}')
        profile = os.environ.get('AWS_PROFILE') or None
        session = boto3.Session(region_name=region, profile_name=profile)
        ssm = session.client('ssm')
        instance_id = '{{ ansible_host }}'

        # Check if agent is registered and online
        resp = ssm.describe_instance_information(
            Filters=[{'Key': 'InstanceIds', 'Values': [instance_id]}]
        )
        info = resp.get('InstanceInformationList', [])
        if info:
            agent = info[0]
            print(json.dumps({
                'PingStatus': agent.get('PingStatus'),
                'AgentVersion': agent.get('AgentVersion'),
                'IsLatestVersion': agent.get('IsLatestVersion'),
                'PlatformName': agent.get('PlatformName'),
                'LastPingDateTime': str(agent.get('LastPingDateTime')),
                'IPAddress': agent.get('IPAddress'),
                'ResourceType': agent.get('ResourceType'),
            }, indent=2))
        else:
            print('NO INSTANCE FOUND in SSM inventory!')
        PYEOF
      args:
        executable: /bin/bash
      register: ssm_agent_status
      changed_when: false
      failed_when: false

    - name: "DEBUG: Show SSM agent status"
      delegate_to: localhost
      become: false
      ansible.builtin.debug:
        msg: "{{ ssm_agent_status.stdout | default('no stdout') }}\nSTDERR: {{ ssm_agent_status.stderr | default('none') }}"

    - name: "DEBUG: Test SSM start-session directly"
      delegate_to: localhost
      become: false
      ansible.builtin.shell: |
        echo "AWS_PROFILE=$AWS_PROFILE"
        echo "Testing aws ssm start-session..."
        aws ssm start-session \
          --region {{ hostvars['localhost']['aws_region'] | default('us-west-2') }} \
          --target {{ ansible_host }} \
          --document-name AWS-StartSSHSession \
          --parameters "portNumber=22" &
        SSM_PID=$!
        sleep 5
        kill $SSM_PID 2>/dev/null || true
        wait $SSM_PID 2>/dev/null || true
        echo "SSM exit code: $?"
      args:
        executable: /bin/bash
      register: ssm_debug_result
      changed_when: false
      failed_when: false

    - name: "DEBUG: Show SSM test result"
      delegate_to: localhost
      become: false
      ansible.builtin.debug:
        msg: "STDOUT: {{ ssm_debug_result.stdout | default('none') }}\nSTDERR: {{ ssm_debug_result.stderr | default('none') }}"

    - name: "DEBUG: Wait 30s then retry SSM (agent recovery test)"
      delegate_to: localhost
      become: false
      ansible.builtin.shell: |
        echo "Waiting 30 seconds for potential agent recovery..."
        sleep 30
        echo "Retrying SSM start-session..."
        aws ssm start-session \
          --region {{ hostvars['localhost']['aws_region'] | default('us-west-2') }} \
          --target {{ ansible_host }} \
          --document-name AWS-StartSSHSession \
          --parameters "portNumber=22" &
        SSM_PID=$!
        sleep 5
        kill $SSM_PID 2>/dev/null || true
        wait $SSM_PID 2>/dev/null || true
        echo "SSM exit code after 30s wait: $?"
      args:
        executable: /bin/bash
      register: ssm_retry_result
      changed_when: false
      failed_when: false

    - name: "DEBUG: Show SSM retry result"
      delegate_to: localhost
      become: false
      ansible.builtin.debug:
        msg: "STDOUT: {{ ssm_retry_result.stdout | default('none') }}\nSTDERR: {{ ssm_retry_result.stderr | default('none') }}"
    # --- END TEMPORARY DEBUG ---

    - name: Wait for SSH connection to stabilize
      ansible.builtin.wait_for_connection:
        timeout: 120
        delay: 10
        sleep: 5

    - name: Gather facts
      ansible.builtin.setup:

    - name: Wait for apt to be ready
      ansible.builtin.shell: |
        while fuser /var/lib/dpkg/lock-frontend /var/lib/apt/lists/lock /var/cache/apt/archives/lock >/dev/null 2>&1; do
          sleep 5
        done
      args:
        executable: /bin/bash
      register: apt_lock_result
      changed_when: false
      retries: 30
      delay: 10
      until: apt_lock_result.rc == 0

    - name: Update apt cache
      ansible.builtin.apt:
        update_cache: true
        cache_valid_time: 3600

    - name: Upgrade all packages
      ansible.builtin.apt:
        upgrade: dist
        autoremove: true

    - name: Check if reboot is required
      ansible.builtin.stat:
        path: /var/run/reboot-required
      register: reboot_required_file

  tasks:
    - name: Configure dev tools
      ansible.builtin.include_tasks: tasks/configure_dev_tools.yml

  post_tasks:
    - name: Display completion message
      ansible.builtin.debug:
        msg: |
          AWS Server provisioning and configuration complete!

          Server Information:
          - Instance: {{ hostvars['localhost']['aws_instance_name'] | default('N/A') }}
          - Instance ID: {{ hostvars['localhost']['aws_instance_id'] | default('N/A') }}
          - IP Address: {{ hostvars['localhost']['aws_instance_public_ip'] | default(ansible_host) }}
          - Access Mode: {{ hostvars['localhost']['aws_access_mode'] | default('ssm') }}
          - Storage: {{ 'EBS ' + hostvars['localhost']['aws_ebs_volume_id'] | default('') if remo_volume_device else 'root volume' }}
          - Region: {{ hostvars['localhost']['aws_region'] | default('us-west-2') }}

          Installed:
          - Docker (with docker-compose plugin)
          - User 'remo' with sudo NOPASSWD access
          - User 'remo' added to docker group
          - Node.js LTS (v{{ nodejs_version }})
          - Git
          - fzf (fuzzy finder for TUI menu)
          - @devcontainers/cli
          - GitHub CLI (gh)
          - Zellij terminal multiplexer

          Login Experience:
          - Project menu auto-starts on SSH login
          - Select projects with arrow keys
          - Devcontainer projects auto-start their container
          - Non-devcontainer projects use zellij sessions

          Security:
          {% if hostvars['localhost']['aws_access_mode'] | default('ssm') == 'ssm' %}
          - SSM Session Manager - no inbound ports open
          {% else %}
          - SSH (port 22) - allowed from your current IP
          - All other ports - blocked
          {% endif %}

          Storage:
          - /home/remo is on {{ 'EBS (block storage)' if remo_volume_device else 'the root volume' }}
          {% if remo_volume_device %}
          - Data persists when instance is terminated
          {% else %}
          - Data does not persist after instance termination
          {% endif %}

          Connect using:
            remo shell
          {% if hostvars['localhost']['aws_access_mode'] | default('ssm') != 'ssm' %}
            ssh remo@{{ hostvars['localhost']['aws_instance_public_ip'] | default(ansible_host) }}
          {% endif %}

    - name: Reboot server if required by package updates
      ansible.builtin.reboot:
        msg: "Rebooting to apply system updates"
        reboot_timeout: 300
      when: reboot_required_file.stat.exists | default(false)

    - name: Display reboot status
      ansible.builtin.debug:
        msg: "{{ 'Server was rebooted to apply system updates.' if reboot_required_file.stat.exists | default(false) else 'No reboot was required.' }}"
